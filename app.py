import streamlit as st
from anthropic import Anthropic
from sentence_transformers import SentenceTransformer
import faiss, numpy as np, os, json, hashlib, time, glob

st.set_page_config(page_title="Ø¦ÛŽØ¬ÛŒÛ†Ú©Û•ÛŒØª (Chomani)", page_icon="ðŸ“š")
st.title("ðŸ“šðŸ’¬ Ú†Ø§ØªØ¨Û†ØªÛŒ Ø¦ÛŽØ¬ÛŒÛ†Ú©Û•ÛŒØª")
st.caption("ØªØ§ÛŒØ¨Û•Øª Ø¨Û• Ø®ÙˆÛŽÙ†Ø¯Ù†ÛŒ Ø®Û†Ú•Ø§ÛŒÛŒ Ùˆ Ø³Ú©Û†ÚµÛ•Ø±Ø´ÛŒÙ¾")

CLAUDE_API_KEY = st.secrets["ANTHROPIC_API_KEY"]
client = Anthropic(api_key=CLAUDE_API_KEY)

KNOWLEDGE_DIR = "knowledge"
os.makedirs(KNOWLEDGE_DIR, exist_ok=True)

EMB_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
EMB_DIM = 384
DEFAULT_MODEL = "claude-sonnet-4-5-20250929"
TOP_K = 10
CHUNK_SIZE = 600
OVERLAP = 200

def hash_docs(doc_paths):
    h = hashlib.sha256()
    for p in sorted(doc_paths):
        h.update(p.encode())
        try:
            h.update(str(os.path.getmtime(p)).encode())
            h.update(str(os.path.getsize(p)).encode())
        except Exception:
            pass
    return h.hexdigest()

def load_text(path):
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):
    words = text.split()
    chunks, start = [], 0
    while start < len(words):
        end = min(start + chunk_size, len(words))
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        if end == len(words): break
        start = max(0, end - overlap)
    return chunks

def build_or_load_index():
    paths = glob.glob(os.path.join(KNOWLEDGE_DIR, "*.txt"))
    if not paths:
        st.warning("âš ï¸ Ù‡ÛŒÚ† Ù¾Û•Ú•Ú¯Û•ÛŒ .txt Ù†Û•Ø¯Û†Ø²Ø±Ø§ÛŒÛ•ÙˆÛ• Ù„Û• Ù¾Û•Ú•Ú¯Û•ÛŒ 'knowledge/' â€” ØªÚ©Ø§ÛŒÛ• Ù¾Û•Ú•Ú¯Û• Ø²ÛŒØ§Ø¯ Ø¨Ú©Û•.")
        return None, None

    doc_hash = hash_docs(paths)
    meta_path = "index_meta.json"
    index_path = "index.faiss"

    if os.path.exists(index_path) and os.path.exists(meta_path):
        try:
            with open(meta_path, "r") as f:
                meta = json.load(f)
            if meta.get("doc_hash") == doc_hash:
                index = faiss.read_index(index_path)
                return index, meta["items"]
        except Exception:
            pass

    st.info("ðŸ—ï¸ Ø¯Ø±ÙˆØ³ØªÚ©Ø±Ø¯Ù†ÛŒ Ø¦ÛŒÙ†Ø¯Û•Ú©Ø³ ... ØªÚ©Ø§ÛŒÛ• Ú†Ø§ÙˆÛ•Ø±ÛŽÚ©Û•")
    emb = SentenceTransformer(EMB_MODEL_NAME)
    items = []
    for p in paths:
        txt = load_text(p)
        for ch in chunk_text(txt):
            items.append({"text": ch, "source": os.path.basename(p)})

    texts = [i["text"] for i in items]
    vecs = emb.encode(texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)
    index = faiss.IndexFlatIP(EMB_DIM)
    index.add(vecs)
    faiss.write_index(index, index_path)

    meta = {"doc_hash": doc_hash, "items": items}
    with open(meta_path, "w") as f:
        json.dump(meta, f)

    st.success("âœ… Ø¦ÛŒÙ†Ø¯Û•Ú©Ø³ Ø¯Ø±ÙˆØ³ØªÚ©Ø±Ø§")
    return index, items

def retrieve(index, items, query, k=TOP_K):
    emb = SentenceTransformer(EMB_MODEL_NAME)
    q_vec = emb.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    D, I = index.search(q_vec, k)
    return [items[i] for i in I[0] if 0 <= i < len(items)]

def ask_claude(question, context, model_id=DEFAULT_MODEL, temperature=0.7, max_tokens=2200):
    system_msg = (
        "ØªÛ† ÛŒØ§Ø±ÛŒØ¯Û•Ø¯Û•Ø±ÛŒ Ø²Ø§Ù†Ø³ØªÛŒ Ùˆ Ø®ÙˆÛŽÙ†Ø¯Ù†ÛŒØª Ø¨Û• Ø²Ù…Ø§Ù†ÛŒ Ú©ÙˆØ±Ø¯ÛŒ (Ø³Û†Ø±Ø§Ù†ÛŒ). "
        "ÙˆÛ•ÚµØ§Ù…Û•Ú©Ø§Ù†Øª Ø¯Û•Ø¨ÛŽØª Ø¨Û• ÙˆØ±Ø¯Û•Ú©Ø§Ø±ÛŒ Ùˆ Ø²Ø§Ù†Ø³ØªÛŒ Ø¨Ù†ÙˆÙˆØ³ÛŒØª. "
        "ØªÛ•Ù†Ù‡Ø§ Ù„Û•Ø³Û•Ø± Ø²Ø§Ù†ÛŒØ§Ø±ÛŒÛ•Ú©Ø§Ù†ÛŒ Ù†Ø§Ùˆ Ú©Û†Ù†ØªÛŽÚ©Ø³Øª ÙˆÛ•ÚµØ§Ù… Ø¨Ø¯Û•. "
        "Ø¦Û•Ú¯Û•Ø± Ø²Ø§Ù†ÛŒØ§Ø±ÛŒÛŒÛ•Ú©Û• Ù†Û•Ø¨ÙˆÙˆØŒ Ø¨ÚµÛŽ 'Ù„Û• Ø¨Ù†Ú©Û•ÛŒ Ø²Ø§Ù†ÛŒØ§Ø±ÛŒ Ù†Û•Ø¯Û†Ø²Ø±Ø§ÛŒÛ•ÙˆÛ•'."
    )
    user_msg = (
        f"### Ú©Û†Ù†ØªÛŽÚ©Ø³Øª:\n{context}\n\n"
        f"### Ù¾Ø±Ø³ÛŒØ§Ø±:\n{question}\n\n"
        "### Ú•ÛŽÙ†Ù…Ø§ÛŒÛŒ:\n"
        "- Ø¨Û• Ø²Ù…Ø§Ù†ÛŒ Ú©ÙˆØ±Ø¯ÛŒ Ø³Û†Ø±Ø§Ù†ÛŒ ÙˆÛ•ÚµØ§Ù… Ø¨Ø¯Û•.\n"
        "- Ø¨Û• Ú•ÙˆÙˆÙ†ÛŒ Ùˆ Ø¨Û• ÙˆØ±Ø¯Û•Ú©Ø§Ø±ÛŒ Ø¨Ù†ÙˆÙˆØ³Û•.\n"
    )
    msg = client.messages.create(
        model=model_id,
        max_tokens=max_tokens,
        temperature=temperature,
        system=system_msg,
        messages=[{"role": "user", "content": user_msg}],
    )
    return msg.content[0].text

with st.spinner("Ø¦Ø§Ù…Ø§Ø¯Û•Ú©Ø±Ø¯Ù†ÛŒ Ø¨Ù†Ú©Û•ÛŒ Ø²Ø§Ù†ÛŒØ§Ø±ÛŒ..."):
    index, items = build_or_load_index()

st.subheader("ðŸ’¬ Ù¾Ø±Ø³ÛŒØ§Ø± Ø¨Ú©Û•")
question = st.text_area("Ù¾Ø±Ø³ÛŒØ§Ø±Û•Ú©Û•Øª Ø¨Ù†ÙˆÙˆØ³Û• Ø¯Û•Ø±Ø¨Ø§Ø±Û•ÛŒ Ø®ÙˆÛŽÙ†Ø¯Ù† ÛŒØ§Ø®ÙˆØ¯ Ú•ÛŽÙ†Ù…Ø§ÛŒÛŒÛ•Ú©Ø§Ù†ÛŒ Ø®ÙˆÛŽÙ†Ø¯Ù†:")

if st.button("Ù†Ø§Ø±Ø¯Ù† / Send", disabled=index is None):
    if not question.strip():
        st.warning("ØªÚ©Ø§ÛŒÛ• Ù¾Ø±Ø³ÛŒØ§Ø± Ø¨Ù†ÙˆÙˆØ³Û•.")
    elif index is None:
        st.error("Ø¨Ù†Ú©Û•ÛŒ Ø²Ø§Ù†ÛŒØ§Ø±ÛŒ Ø¦Ø§Ù…Ø§Ø¯Û• Ù†ÛŒÛŒÛ•.")
    else:
        with st.spinner("Ú¯Û•Ú•Ø§Ù† Ù„Û• Ù†Ø§Ùˆ Ø¨Ù†Ú©Û•ÛŒ Ø²Ø§Ù†ÛŒØ§Ø±ÛŒ..."):
            hits = retrieve(index, items, question)
            context = "\n\n".join([h["text"] for h in hits])
        with st.spinner("ÙˆÛ•ÚµØ§Ù…Ø¯Ø§Ù†Û•ÙˆÛ• Ù„Û•Ù„Ø§ÛŒÛ•Ù† Claude..."):
            try:
                answer = ask_claude(question, context)
                st.markdown("### ðŸ§  ÙˆÛ•ÚµØ§Ù…:")
                st.write(answer)
            except Exception as e:
                st.error(f"âŒ Ù‡Û•ÚµÛ•ÛŒÛ•Ú© Ú•ÙˆÙˆÛŒ Ø¯Ø§: {e}")
